\documentclass[dvipdfm, NoBUAAtitle]{BUAApaper}
\begin{document}
\title{外文文献翻译：Implementation Experiences in Transparently Harnessing Cluster-Wide Memory}
\author{Michael R. Hines, Mark Lewandowski and Kartik Gopalan\\翻译：张三}
\date{200X年6月}
\maketitle

\section*{摘要}

即使~DRAM~容量持续增长，内存容量和高性能计算任务对内存容量的需求总是一对永恒的矛
盾。一旦系统达到了物理内存的限制，这些应用便开始磁盘交换，性能急剧退化。随着支持
巨帧的商用~gigabit Ethernet Lan~的普及，现在是时候考虑一种一直没有得到应得的重视
的解决方案。最近，我们对将结点的内存资源用高速网络连接起来所得到的收益和付出的代
价进行了调研。我们提出了设计，实现并且分析了~{\em Anemone}──一个网络内存引擎，
它将多个机器中空闲的内存通过高速网络集合起来并虚拟化，并且不需要对消耗内存的应用
程序做任何修改。我们实现了一个可用的~Anemone~原形，并且用实际的、未加修改的应用
程序，例如光线跟踪、在内存中大规模的排序对它进行了测试分析。我们得到了这样的结论：
使用~Anemone~访问远程内存和基于磁盘的分页相比，未经修改的单线程应用程序的执行
效率达到了2到3倍，多线程并发执行最大达到了7.7倍。Anemone~降低了页面错误的延迟，
达到了19.6倍──磁盘的平均延迟为9.8毫秒，而~Anemone~则是500微妙。更重要的，
Anemone~提供了一个通过网络虚拟化的低延迟内存访问，而潜在的容量是``无限''。

\section{简介}

DRAM~技术的高速增长引起了通用操作系统(off-the-shelf system)不可预测的内存容量增
加。同时，应用程序对内存的需求也日益增长，例如多媒体、图像处理，高分辨率立体渲染、
天气预报、大规模仿真、巨型数据库，内存容量如何增长似乎也满足不了它们无尽的需求。
结果，虽然结点上的内存越来越多，但与其说现在已经可以题供足够的物理内存去满足现
代这些内存饥饿的应用程序，到不如说我们提供的越多，它们越块的把内存耗尽然后要求更
多。这场持久战双方总是战成平手，大内存需求的应用程序总是很快的达到内存容量限制然
后开始向磁盘交换数据。在分页过程中，系统将缓慢的本地磁盘看成是物理内存的扩展，向
它们写入和读取进程虚拟内存空间的超量信息，应用程序的瓶颈仍然在磁盘访问延迟上。

同时，总是有这种情况：当一个结点上的内存严重不足时，令一个结点上却有大量内存空闲
着，而它们又在同一个高速网络中。在通常的集群系统中，资源的利用率在~5\%~到~20\%~
左右。因此，和直接向一个慢速磁盘分页相比，如果先从高速网络向其它结点的空闲内存分
页，而把向磁盘分页当成是远程内存耗尽后的最终措施，就可以显著的减少访问延迟。如图
一(图略)所示，远程内存访问可以看做是传统的内存层次结构中的另一级，它填补了低延迟
的本地物理内存和高延迟的本地磁盘性能上的巨大鸿沟。实际上，远程内存分页很容易达到
600微妙或更低的延迟，反之缓慢的本地磁盘的延迟根据寻道和转速的开销不同总是在6到
13毫秒之间。因此，远程内存分页潜在的比本地磁盘分页块两个数量级。

以上讨论很自然的引出了一个有趣的问题：我们是否可以透明的将以高速网络互连的各结点
的空闲内存聚合在一起，使得未经修改的大内存需求的应用程序通过使用这些内存避免磁盘
访问瓶颈？前人的一些研究[7, 13, 12, 19, 21, 14, 22, 26]已经指出了这个问题，但是
对于通过网络开发内存资源的关注程度还不够。主要是因为先前的研究者要么依靠昂贵的高
速硬件，例如~ATM~和~Myrinet~交换机，要么使用带宽有限的~10Mbps~或~100Mbps~的网络
，这些网络对于产生有意义的应用程序加速而言太慢了。另外，先前的研究经常需要对应用
程序本身或操作系统甚至两者进行大幅度的修改。当一些制造商用特殊的设备建立了有大内
存的服务器后，这些解决方案就显得太昂贵了，技术本身也很快过时了。请注意以上的研究
和分布式共享内存系统[10]不同，后者允许网络中的结点像共享内存的多处理器，通常需要
定制的编程接口。

近年来，普及型的~gigabit Ethrenet LAN~有了惊人的增长，它们可以提供第延迟，支持巨
帧(超过1500字节的包)，且性价比非常有吸引力。本文重新研究了通过~gigabit Ethrenet
LAN~透明的远程内存访问的可行性。更进一步，我们的目标是虚拟化集群中的空闲内存以支
持消耗内存的应用程序而不必修改应用程序。虚拟化指的是将一组物理资源包括计算能力、
内存、存储空间、带宽，从更高层聚集并重新划分为逻辑的各部分。如同操作系统在单个机
器上无缝的将多种硬件为每个执行中的进程虚拟化，我们的目标是用一种透明的方法将集群
中的内存资源虚拟化。

我们在这里提供了我们在设计、实现、评估~{\bf\slshape Adaptive Network Memory
Engine(Anemone)}~系统时的经验。Anemone~将用~gigabit Ethrenet LAN~连接起来的集群
中各结点的内存资源组成一个池进行虚拟化，为大内存需求的应用程序提供透明的、低开销
的内存访问，以及潜在的``无限''的远程内存资源。Anemone~系统有以下贡献：

\begin{itemize}

\item 就我们所知，Anemone~是第一次在支持巨帧的商用~gigabit Ethrenet LAN~上进行远
程内存访问的可行性研究；

\item Anemone~为每一个内存客户机访问潜在的聚集了无限内存的池提供了统一的~{\em 虚
	拟化接口}；

\item 所有应用程序都可以从~Anemone~中获益，而不需要代码的修改、重新编译或重新连
接，消除了代价高昂的对最终系统的修改。Anemone~系统是可插入的内核模块，不需要对操
作系统核心进行任何修改；

\item Anemone~的设计可以自动的适应应用程序内存需求的变化，和/或全集群中可用内存
资源的变化。它将客户程序有效的与聚集内存池的变化隔离，同时将内存的贡献者和客户内
存需求隔离。

\end{itemize}

我们用真实世界中未经修改的应用程序对~Anemone~的性能进行了测试：光线跟踪和大规模
内存中排序。Anemone~降低页面错误的延迟达19倍：从平均9.8毫秒到500微秒。Anemone~使
单进程、大内存需求的应用程序的性能提升了2到3倍，使多进程并发大内存需求的性能提升
了2到6倍。我们的经验说明要达到这样的性能必须在交换空间和~Anemone~客户端模块之间
实现有效的流控制、预测页缓存以及设计底层轻量级可靠通信协议。我们对多进程应用的性
能分析倾向于说明，当问题的规模增加时，通过~Anemone~可以得到更大的加速。

本文其余部分是如下组织的。第2章给出了~Anemone~系统的结构设计以及理论基础。第4章
给出了~Anemone~详细的实现。第5章给出了可用的~Anemone~原形的性能测试结果。第6章将
~Anemone~与先前的类似系统进行比较。第8章对我们的工作进行总结，并给出未来发展的方
向。

\section{Anemone~的设计选择}

本章检查了分布式远程内存访问可能的设计。我们检查了多种设计选择，考虑了代价以及影
响这个系统的实际理论。我们最终得到了一个可以在本文和第一个原形系统中实现的子集。

\begin{description}

\item [(A) 透明的虚拟化] Anemone~的设计首先要考虑的是使得内存密集型的应用程序透
明的从远程内存资源中获益，同时不须修改、重新编译或重新连接。实现这个目标的第一个
选择是修改操作系统内核的虚拟内存子系统。当本地内存不足时，页表映射可以首先指向远
程内存页面，当远程内存页面不足时，页表映射再直接指向本地硬盘。虽然对应用程序本身
是透明的，这种解决方案修改了操作系统内核。第二种虚拟化方案是提供一个或多个{\em 
伪块设备}。每个伪块设备可以被用作交换空间，如同主交换空间，在这些设备中，页面交
换到远程内存中而不是本地磁盘中。这种解决方案可以简单的作成自包含的内核模块，不需
要修改操作系统核心，且允许交换机制处理所有和分页有关的事项(磁盘和远程内存)。因为
以上原因，我们采纳第二种解决方案访问远程内存。另外，伪块设备的接口可以被看做低延
迟的磁盘，可以保存临时文件以便快速访问，或被一些对远程内存感兴趣的应用程序使用。
虽然伪块设备可以被用做其它用途，本文主要关注它作为交换空间的情况。

\item[(B) 远程内存访问协议(RMAP)] 多个设计要求需要指定访问远程内存的协议。第一个
要求是确保协议处理的开销不能在访问远程页面的总延迟中占优。可以专门的在~TCP, UDP~
甚至直接在~IP~上实现~RMAP，但是这样会造成不期望的协议处理开销。一个低开销的解决
方案需要实现一个轻量级的通信协议，可以直接在网络驱动程序上层执行，没有任何中间的
协议层。这个设计和内存页面大小、~LAN~最大帧长度(MTU)有关。一般的页面大小在多数操
作系统中是4KB(有些是8KB)，而传统的以太网~MTU~是1500字节。于是~4KB~的内存页面被迫
分成3片发送，在接收端重组，增加了很大开销。不过可以利用~Gigabit LANs~的巨帧特性
。在巨帧中包大小可以超过1500字节(一般在~8KB~到~16KB~之间)。巨帧使得~RMAP~可以通
过一个单独的包将完整的~4KB~页传输到远程，没有分片的开销。第三个要考虑的是~RMAP~
的可靠性。在重负载下硬件如网卡、交换机丢弃帧是很常见的现象。因此~RMAP~需要是一个
可靠的协议，因为在交换中丢掉应用程序的内存页面会引发应用程序的崩溃，这是不可接受
的。RMAP~同时需要流控制以确保接收者以及网络中的网卡、交换机不会过载。但是，
RMAP~不需要传统的~TCP~的一些特性，如字节流抽象化、按序分发和拥塞控制等。因此我们
决定将~RMAP~实现成一个轻量级基于窗口的数据报协议。

\item[(C) 中央极权 vs. 分布式资源管理] 和所有分布式系统一样，Anemone~面临着是向
一个中央管理结点从全集内存池中申请内存还是按某种分布式的协议从所有参与结点中申请
内存。中央极权式的模型实现起来更简单，它的优点在于一个单独的实体了解~LAN~中全集
内存分布情况，因此进行资源管理时可以更有效。明显的缺点在于，中央结点将成为瓶颈，
且一旦崩溃就造成系统崩溃，即使在一般的操作中，单结点在内存访问方面引入了额外的一
层。一个仔细设计的分布式管理模型不会有这些问题，但是需要结点间更多的协作，以更有
效的利用全局内存资源的。不过，额外的协作不一定意味着高代价。我们相信考虑到长远的
可扩展性、性能和安全性，分际降哪Ｐ透谩nemone~系统直接、现实的考虑是使用通过
~Gigabit Ethrenet~技术访问远程内存是否可行。不过，作为原形系统，为简单和高速起见，
原形系统使用中央管理的内存引擎，作为~Client~和~Server~之间的媒介。另外，我们的设
计有足够的灵活性，以便下一步使用分布式的内存管理。

\item[(D) 多路复用] 在有多个远程内存消费者(客户)和远程内存提供者(服务器)的网络中，
支持两种多路复用是很重要的：(a) 任何一个客户必须可以从多个服务器中获得内存，并且
透明的通过一个伪块设备接口访问，如同从池中访问；(b) 每一个服务器必须有能力并发的
为多个客户端提供内存。这样可以为有效的使用全局内存池提供最大的灵活性，通过一个服
务器为所有客户端服务阻止资源的浪费。

\item[(E) 负载均衡] 网络中的内存提供者和其它一般结点可能有它们自己的进程和内存需
求。因此~Anemone~系统的另一个设计目标是尽量阻止任何一个内存服务器的过载。这需要
尽可能的将客户端的负载分担给后端服务器。在中央管理式的模型中，这个功能是由内存引
擎提供的，因问它可以跟踪内存服务器的负载。分布式模型则要求~Server~和~Client~之间
额外的协调。

\item[(F) 适应资源变化] 随着内存提供者持续的加入或离开网络，Anemone~系统面临者以
下设计需求：(a) 需要无缝的处理集群范围的内存容量变化，将内存客户与资源的变动隔离
开；(b) 需要允许任何服务器通过透明的将页迁移到低负载的结点上的方法将它贡献的部分
或全部内存收回。因此~Anemone~需要随着聚集内存空间容量的增长、缩小而动态的变化；
同时使用磁盘作为后备资源，防止远程内存池过载。

\item[(G) 容错] 另一方面，远程分页错误最严重也就是和本地磁盘分页错误一样，导致应
用程序崩溃。同时，因为参与的组件多，在~LAN~环境中发生错误的可能性更大。网卡、接
口、电缆、交换机和远程内存服务器自己都可能出错。因此~Anemone~的设计对可靠性的要
求就要高于本地磁盘分页。有两个可行的容错技术，二者都引入了冗余数据。最简单的解决
方案是为每一个交换远程的内存页面维护一个基于本地磁盘的备份。它的可靠性就是本地磁
盘分页的可靠性。但是因为本地磁盘的存在会影响性能。另一种解决方案是在多个服务器上
保存冗余。这种解决方案避免了磁盘的活动，但是消耗了额外的网络带宽，降低了全局内存
池的性能，而且依然对网络的可靠性敏感。两种选择中，维护多个冗余的一致性也是一个问
题。我们在~Anemone~中倾向于第一种方案，因为它更简单，和恶意或非恶意的数据损失相
比更安全，而且磁盘错误的发生频率低于网络错误。

\item[(H) 安全性] 现在将应用程序的内存存到了第三方结点上，因此确保安全和数据完整
性现在成为了重要的问题。数据完整性本身可以使用加密和校验来保证，虽然客户端引入了
额外的计算开销。恶意的数据丢失和欺骗可以通过在本地磁盘上保留备份解决(如上文述)。

\item[总结和本文重点] 我们在本文中对~Anemone~最初的原形系统的描述主要关注于以上
的(A)--(E)。当我们论证了高性能远程内存访问的可行性和可用性后，我们未来的工作是实
现如分布式资源管理、适应性、容错性和安全性。

\end{description}

\section{Anemone~的结构}

本章描述了~Anemone~系统的详细结构以及如何体现上一章说明的设计目标。Anemone~系统
有3个主要组件：内存引擎、内存服务器和内存客户。图2(图略)说明了这些组件和它们的关
系。

\subsection{内存引擎}

内存引擎是专注于全局内存资源管理的专门结点，协调客户端大内存应用程序和提供远程内
存的服务器之间的交互。选则中央式的体系结构是为了证明可行性，是全分布式体系结构的
前身。在后者中，内存引擎的功能将被每个客户和服务器包含。内存引擎本身不是主要的远
程内存源，它为对远程内存资源的访问提供了统一的访问接口。内存服务器将客户端的内存
需求透明的映射到可用的远程内存，对前端的应用和后端的服务器隐藏了内存资源管理的复
杂性。内存引擎使用轻量级基于窗口的可信内存访问协议(RMAP)和客户端、服务器通信。

在客户端看来，内存引擎由一组虚拟访问接口(VAI)构成，它们是一组逻辑设备而不是物理
设备，可以被客户端用于访问远程内存的内容。从客户端来的内存访问需求以读/写请求的
形式向这个远程的~VAI~设备发送。引擎有能力同时支持数个集群中客户的~VAI~请求。同时，
它也允许多个内存服务器将不用的内存空间贡献给集群中的多个客户。每一个~VAI~可以
有诸如逻辑容量、内存服务器列表、可靠性要求等等属性，并且和其它~VAI~有优先级的关
系。内存引擎存储在内存中这些特性，提供给用户一组~VAI~处理函数，客户端可以和
~VAI~交互。

紧接着一次写，客户端模块将页面发送给引擎，接下来这个页面被存储在一个后端内存服务
器上。后续的页面，包括同一个客户端的页面，可以被保存在任何内存服务器上。这使得内
存引擎在实现各种服务器选择算发上有可伸缩性。紧接着一次读，客户发送读请丢给引擎，
引擎检索它的内部映射表以确定存放着请求页面的服务器，从那个服务器中找回页面，将它
传给客户端。在每个~VAI~的页面映射信息之外，内存引擎维护一个缓存，它保存了最经常
使用的页面。在一个从任何一个~VAI~的客户端发起的读请求后，内存引擎首先寻找它的缓
存，仅当缓存不命中时再访问内存服务器。整个过程没有引入磁盘访问，所有操作都完全在
内存中进行。

内存引擎在~Anemone~的结构中处在中心的地位，它隐藏了所有从客户端映射分布式内存资
源的复杂性，以及管理向内存服务器的客户请求的复杂性。客户视角的~VAI~透明的虚拟化
了集合内存自晕。客户端简单的和~VIA~交互，内存引擎隐藏了细节，且有效的管理了底层
复杂的虚拟化。

\end{document}
